# ğŸš€ E-Commerce Real-Time Streaming Pipeline

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![Spark](https://img.shields.io/badge/Spark-3.5.0-orange.svg)](https://spark.apache.org/)
[![Kafka](https://img.shields.io/badge/Kafka-7.4.0-black.svg)](https://kafka.apache.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-blue.svg)](https://www.postgresql.org/)

Production-ready streaming data pipeline untuk real-time e-commerce analytics dengan fokus pada **Funnel Analysis**, **GMV Tracking**, dan **Drop-off Detection**.

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Quick Start](#-quick-start)
- [Project Structure](#-project-structure)
- [Usage](#-usage)
- [Monitoring](#-monitoring)
- [Interview Q&A](#-critical-interview-questions)
- [Development](#-development)
- [Troubleshooting](#-troubleshooting)

## âœ¨ Features

### Real-Time Analytics
- âœ… **Real-time Funnel Analysis** - Track user journey: Order â†’ Items â†’ Payment
- âœ… **GMV Metrics** - Gross Merchandise Value tracking per minute/hour
- âœ… **Drop-off Detection** - Identify where users abandon transactions
- âœ… **Payment Analytics** - Payment method insights and patterns

### Production-Ready Architecture
- âœ… **Fault Tolerance** - Spark checkpointing + Kafka offset management
- âœ… **Exactly-Once Semantics** - Event deduplication via event_id
- âœ… **Late Data Handling** - Watermarking for out-of-order events
- âœ… **Scalability** - Partitioned topics + stateful processing
- âœ… **Monitoring** - Grafana dashboards + Prometheus metrics

### Code Quality
- âœ… **Modular Design** - Separated concerns (config, processors, sinks)
- âœ… **Schema Validation** - Avro schemas for data quality
- âœ… **Error Handling** - Comprehensive error handling and logging
- âœ… **Type Safety** - Type hints throughout codebase

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CSV Dataset  â”‚ (Brazilian E-Commerce)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Event Generator (Producer)                â”‚
â”‚  - Replay historical data as real-time events         â”‚
â”‚  - Configurable speed (1x, 10x, 100x)                â”‚
â”‚  - Schema validation (Avro)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Apache Kafka (3 topics)                  â”‚
â”‚  - olist.orders       (Order events)                  â”‚
â”‚  - olist.order_items  (Item events)                   â”‚
â”‚  - olist.payments     (Payment events)                â”‚
â”‚                                                       â”‚
â”‚  Features: Partitioning, Retention, Compression       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Spark Structured Streaming                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Stream Processors (Modular)                   â”‚    â”‚
â”‚  â”‚  - FunnelProcessor    (Conversion tracking)   â”‚    â”‚
â”‚  â”‚  - GMVProcessor       (Revenue metrics)       â”‚    â”‚
â”‚  â”‚  - DropOffProcessor   (Abandonment detection) â”‚    â”‚
â”‚  â”‚  - PaymentProcessor   (Payment analytics)     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                       â”‚
â”‚  Features: Deduplication, Watermarking, Joins         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Sinks (Multiple outputs)                 â”‚
â”‚  - PostgreSQL (Persistent metrics)                    â”‚
â”‚  - Console    (Real-time monitoring)                  â”‚
â”‚  - Prometheus (System metrics)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Grafana Dashboards                         â”‚
â”‚  - Real-time funnel visualization                     â”‚
â”‚  - GMV trends and forecasting                         â”‚
â”‚  - Drop-off analysis and alerts                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- Python 3.9+
- Java 11+ (for Spark)
- 8GB RAM minimum
- Dataset from [Kaggle](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)

### 1. Clone and Setup

```bash
# Clone repository
git clone <your-repo-url>
cd ecommerce-streaming-pipeline

python -m venv .venv

#windows
.venv\Scripts\activate

#macos/linux
source .venv/bin/activate

# Setup environment
make setup

# Download dataset and extract to dataset/ folder
# https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce
```

### 2. Start Infrastructure

```bash
# Start all services (Kafka, PostgreSQL, Grafana, etc.)
make infra-up

# Verify services are running
make status
```

### 3. Run Pipeline

**Terminal 1 - Start Consumer:**
```bash
# Local mode (development)
make consumer

# Cluster mode (muncul di Spark UI http://localhost:8080)
make consumer-cluster
```

**Terminal 2 - Start Producer:**
```bash
# Test with 500 orders at 5x speed
make producer

# Or run full dataset
make producer-full
```

### 4. Monitor

```bash
# Check service status
make status

# View latest database metrics
make db-latest

# Monitoring URLs:
# Kafka UI:    http://localhost:8090
# Grafana:     http://localhost:3000 (admin/admin)
# Spark UI:    http://localhost:4040 (local mode)
# Spark Master: http://localhost:8080 (cluster mode)
```

## ğŸ“ Project Structure

```
ecommerce-streaming-pipeline/
â”‚
â”œâ”€â”€ config/                      # Configuration layer
â”‚   â”œâ”€â”€ kafka_config.py         # Kafka settings
â”‚   â”œâ”€â”€ spark_config.py         # Spark settings
â”‚   â”œâ”€â”€ database_config.py      # PostgreSQL settings
â”‚   â””â”€â”€ app_config.py           # Application settings
â”‚
â”œâ”€â”€ src/                         # Source code
â”‚   â”œâ”€â”€ producers/              # Event producers
â”‚   â”‚   â”œâ”€â”€ base_producer.py   # Base producer class
â”‚   â”‚   â””â”€â”€ event_generator.py # Main event generator
â”‚   â”‚
â”‚   â”œâ”€â”€ consumers/              # Stream consumers
â”‚   â”‚   â””â”€â”€ stream_processor.py # Main Spark processor
â”‚   â”‚
â”‚   â”œâ”€â”€ processors/             # Business logic (modular)
â”‚   â”‚   â”œâ”€â”€ funnel_processor.py   # Conversion tracking
â”‚   â”‚   â”œâ”€â”€ gmv_processor.py      # Revenue metrics
â”‚   â”‚   â”œâ”€â”€ dropoff_processor.py  # Abandonment detection
â”‚   â”‚   â””â”€â”€ payment_processor.py  # Payment analytics
â”‚   â”‚
â”‚   â”œâ”€â”€ sinks/                  # Data sinks
â”‚   â”‚   â””â”€â”€ metrics_sink.py    # PostgreSQL + Console (unified)
â”‚   â”‚
â”‚   â”œâ”€â”€ schemas/                # Avro schemas
â”‚   â”‚   â””â”€â”€ avro_schemas.py    # Schema definitions
â”‚   â”‚
â”‚   â””â”€â”€ utils/                  # Utilities
â”‚       â””â”€â”€ logger.py          # Logging configuration
â”‚
â”œâ”€â”€ infrastructure/             # Infrastructure as Code
â”‚   â”œâ”€â”€ docker/
â”‚   â”‚   â””â”€â”€ docker-compose.yml # All services
â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â””â”€â”€ setup_topics.sh   # Topic creation
â”‚   â”œâ”€â”€ postgres/
â”‚   â”‚   â””â”€â”€ init.sql          # Database schema
â”‚   â””â”€â”€ grafana/
â”‚       â””â”€â”€ dashboards/       # Pre-built dashboards
â”‚
â”œâ”€â”€ scripts/                    # Operational scripts
â”‚   â””â”€â”€ setup_kafka_topics.sh  # Kafka setup
â”‚
â”œâ”€â”€ dataset/                    # Data files (not in git)
â”œâ”€â”€ checkpoint/                 # Spark checkpoints
â”œâ”€â”€ logs/                      # Application logs
â”‚
â”œâ”€â”€ Makefile                   # Automation commands
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # This file
```

## ğŸ¯ Usage

### Common Commands

```bash
# Setup & Infrastructure
make setup             # Install dependencies
make infra-up          # Start all services
make infra-down        # Stop all services
make status            # Check service status
make logs              # View service logs

# Pipeline
make producer          # Run event generator (500 events, 5x speed)
make producer-full     # Run full dataset
make consumer          # Run stream processor (local mode)
make consumer-cluster  # Run on Spark cluster (muncul di Spark UI)
make consumer-bg       # Run in background

# Database
make db-shell          # PostgreSQL shell
make db-latest         # Show latest metrics
make db-complete       # Complete funnel view (with GMV & payments)
make db-truncate       # Clear all tables

# Cleanup
make clean             # Clean checkpoints & logs
make reset             # Full reset

# Demo
make demo              # End-to-end pipeline demo
```

### Advanced Usage

**Custom Producer Settings:**
```bash
python -m src.producers.event_generator \
    --dataset-dir ./dataset \
    --speed 10 \
    --limit 1000
```

**Custom Consumer Settings:**
```bash
python -m src.consumers.stream_processor \
    --no-postgres \
    --no-console
```

## ğŸ“Š Monitoring

### Kafka UI (Port 8090)
- Topics overview
- Consumer groups
- Message inspection
- Partition details

### Grafana (Port 3000)
Default credentials: `admin/admin`

**Pre-built Dashboards:**
1. **Real-Time Funnel** - Conversion rates and drop-off visualization
2. **GMV Trends** - Revenue tracking and forecasting
3. **Drop-off Analysis** - Abandonment patterns
4. **Payment Insights** - Payment method distribution

### Spark UI (Port 4040)
- Streaming query progress
- Job execution details
- Stage metrics

### PostgreSQL Queries

```sql
-- Latest funnel metrics
SELECT * FROM real_time_funnel 
ORDER BY window_start DESC 
LIMIT 10;

-- Hourly GMV summary
SELECT * FROM hourly_funnel_summary 
ORDER BY hour DESC;

-- Drop-off by status
SELECT 
    order_status,
    SUM(dropped_orders) as total_dropped,
    AVG(dropped_orders) as avg_dropped
FROM drop_off_analysis
GROUP BY order_status;

-- Payment method distribution
SELECT 
    payment_type,
    SUM(payment_count) as total_payments,
    SUM(total_value) as total_revenue
FROM payment_metrics
GROUP BY payment_type
ORDER BY total_revenue DESC;
```

## ğŸ”‘ Critical Interview Questions

### 1. Apa yang terjadi kalau consumer mati?

**Jawaban:**
Spark Streaming menggunakan **checkpointing** untuk menyimpan state dan **Kafka offset management**. Saat consumer restart:

1. Read checkpoint terakhir
2. Resume dari last committed offset
3. Continue processing tanpa data loss

**Bukti dalam kode:**
```python
# config/spark_config.py
CHECKPOINT_DIR = "./checkpoint"

# Setiap query punya checkpoint sendiri
checkpoint_location = SparkConfig.get_checkpoint_location("funnel")
```

### 2. Apakah data hilang?

**TIDAK**, karena:

1. **Kafka Durability**
   - Data persisted di disk
   - Retention 7 hari (configurable)
   - Replication (untuk production)

2. **Spark Checkpointing**
   - State disimpan periodic
   - WAL (Write-Ahead Log)
   - Atomic commit

3. **Deduplication**
   ```python
   def _deduplicate(self, df: DataFrame) -> DataFrame:
       return df.dropDuplicates(["event_id"])
   ```

**Trade-off:** Bisa ada duplicates jika crash sebelum commit, tapi kita handle dengan deduplication.

### 3. Bisa replay event?

**YA, ada 3 cara:**

**Option 1: Reset Offset**
```bash
docker exec kafka kafka-consumer-groups \
    --bootstrap-server localhost:29092 \
    --group ecommerce-streaming-consumer \
    --reset-offsets --to-earliest \
    --all-topics --execute
```

**Option 2: Delete Checkpoint**
```bash
make clean-checkpoints
# Spark akan start dari "earliest"
```

**Option 3: New Consumer Group**
```python
# Ganti group_id di config
CONSUMER_CONFIG = {
    "group_id": "new-consumer-group-v2"
}
```

### 4. Apa dampaknya ke funnel?

**Real-time Funnel Metrics:**

```python
# FunnelProcessor menghitung:
- total_orders              # Stage 1
- orders_with_items         # Stage 2
- orders_with_payment       # Stage 3

# Conversion rates
items_conversion_rate = (orders_with_items / total_orders) Ã— 100
payment_conversion_rate = (orders_with_payment / total_orders) Ã— 100

# Drop-off detection
dropped_after_order = total_orders - orders_with_items
dropped_after_items = orders_with_items - orders_with_payment
```

**Dashboard menampilkan:**
- Real-time conversion rate
- Bottleneck identification
- Trend analysis

### 5. Bagaimana scaling Kafka topic?

**Horizontal Scaling:**

```bash
# Increase partitions
docker exec kafka kafka-topics --alter \
    --bootstrap-server localhost:29092 \
    --topic olist.orders \
    --partitions 6

# Add more consumers (parallel processing)
python -m src.consumers.stream_processor &  # Consumer 1
python -m src.consumers.stream_processor &  # Consumer 2
```

**Key Points:**
- Max parallelism = jumlah partitions
- Partition key: `order_id` (data locality)
- Auto rebalancing saat consumer join/leave

**Scaling Strategy:**
```
1 partition  + 1 consumer  = 1x throughput
3 partitions + 3 consumers = 3x throughput
6 partitions + 6 consumers = 6x throughput
```

## ğŸ› ï¸ Development

### Setup Development Environment

```bash
# Install dependencies
pip install -r requirements.txt
pip install -r requirements-dev.txt  # Testing tools

# Setup pre-commit hooks (optional)
pre-commit install
```

### Running Tests

```bash
# Unit tests
pytest tests/unit/

# Integration tests
pytest tests/integration/

# All tests
pytest tests/
```

### Code Quality

```bash
# Format code
black src/ config/

# Lint
pylint src/ config/

# Type checking
mypy src/ config/
```

## ğŸ› Troubleshooting

### Kafka Connection Issues

```bash
# Check Kafka is running
docker ps | grep kafka

# Check Kafka logs
docker logs kafka

# Test connectivity
make test-producer
```

### Checkpoint Corruption

```bash
# Clean and restart
make clean-checkpoints
make consumer
```

### Out of Memory

```bash
# Increase Spark memory
export SPARK_DRIVER_MEMORY=4g
export SPARK_EXECUTOR_MEMORY=4g

# Or reduce shuffle partitions
# Edit config/spark_config.py:
SHUFFLE_PARTITIONS = 2
```

### PostgreSQL Connection Failed

```bash
# Check PostgreSQL is running
make status

# Test connection
make test-postgres

# Connect to shell
make db-shell
```

## ğŸ“š References

- [Brazilian E-Commerce Dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)
- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- [Spark Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [PostgreSQL Documentation](https://www.postgresql.org/docs/)

**Built with â¤ï¸ for demonstrating production-ready streaming architecture**